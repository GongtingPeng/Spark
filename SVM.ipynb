{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMV with Spark and MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.mllib.classification import SVMModel, SVMWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors as MLLibVectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import * \n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import string \n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load review raw data\n",
    "review = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "                .option(\"uri\", \"mongodb://172.31.40.203:27017/yelp.review\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cache  dataframes\n",
    "review.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "def remove_punct(text):\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", text) \n",
    "    return nopunct\n",
    "    \n",
    "# binarize rating\n",
    "def convert_rating(rating):\n",
    "    rating = int(rating)\n",
    "    if rating >=4: return 1\n",
    "    else: return 0\n",
    "\n",
    "# udf\n",
    "punct_remover = udf(lambda x: remove_punct(x))\n",
    "rating_convert = udf(lambda x: convert_rating(x))\n",
    "\n",
    "# apply to review raw data\n",
    "review_df = review.select('review_id', punct_remover('text'), rating_convert('stars'))\n",
    "\n",
    "review_df = review_df.withColumnRenamed('<lambda>(text)', 'text')\\\n",
    "                     .withColumn('label', review_df[\"<lambda>(stars)\"].cast(IntegerType()))\\\n",
    "                     .drop('<lambda>(stars)')\\\n",
    "                     .limit(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize\n",
    "tok = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "review_tokenized = tok.transform(review_df)\n",
    "\n",
    "# remove stop words\n",
    "stopword_rm = StopWordsRemover(inputCol='words', outputCol='words_nsw')\n",
    "review_tokenized = stopword_rm.transform(review_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count vectorizer\n",
    "cv = CountVectorizer(inputCol='words_nsw', outputCol='tf')\n",
    "cvModel = cv.fit(review_tokenized)\n",
    "count_vectorized = cvModel.transform(review_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "idf = IDF().setInputCol('tf').setOutputCol('tfidf')\n",
    "tfidfModel = idf.fit(count_vectorized)\n",
    "tfidf_df = tfidfModel.transform(count_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into training and testing set\n",
    "splits = tfidf_df.select(['tfidf', 'label']).randomSplit([0.8,0.2],seed=100)\n",
    "train = splits[0].cache()\n",
    "test = splits[1].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMModel, SVMWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors as MLLibVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to LabeledPoint vectors\n",
    "train_lb = train.rdd.map(lambda row: LabeledPoint(row[1], MLLibVectors.fromML(row[0])))\n",
    "test_lb = test.rdd.map(lambda row: LabeledPoint(row[1], MLLibVectors.fromML(row[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM model\n",
    "numIterations = 50\n",
    "regParam = 0.3\n",
    "svm = SVMWithSGD.train(train_lb, numIterations, regParam=regParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "test_lb = test.rdd.map(lambda row: LabeledPoint(row[1], MLLibVectors.fromML(row[0])))\n",
    "scoreAndLabels_test = test_lb.map(lambda x: (float(svm.predict(x.features)), x.label))\n",
    "score_label_test = spark.createDataFrame(scoreAndLabels_test, [\"prediction\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# F1 score\n",
    "f1_eval = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "svm_f1 = f1_eval.evaluate(score_label_test)\n",
    "print(\"F1 score: %.4f\" % svm_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = cvModel.vocabulary\n",
    "weights = svm.weights.toArray()\n",
    "svm_coeffs_df = pd.DataFrame({'word': vocabulary, 'weight': weights})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_coeffs_df.sort_values('weight').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace unigrams with trigrams of top coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "top_ngram = svm_coeffs_df_ngram.sort_values('weight')['ngram'].values[:20]\n",
    "bottom_ngram = svm_coeffs_df_ngram.sort_values('weight', ascending=False)['ngram'].values[:20]\n",
    "ngram_list = list(top_ngram) + list(bottom_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace the word with selected ngram\n",
    "def ngram_concat(text):\n",
    "    text1 = text.lower()\n",
    "    for ngram in ngram_list:\n",
    "        if ngram in text1:\n",
    "            new_ngram = ngram.replace(' ', '_')\n",
    "            text1 = text1.replace(ngram, new_ngram)\n",
    "    return text1\n",
    "\n",
    "ngram_df = udf(lambda x: ngram_concat(x))\n",
    "ngram_df = review_tokenized.select(ngram_df('text'), 'label')\\\n",
    "                          .withColumnRenamed('<lambda>(text)', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_df.filter(ngram_df['text'].contains('great_service_')).select('text').limit(1).rdd.map(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize and remove stop words with ngram\n",
    "tokenized_ngram = tok.transform(ngram_df)\n",
    "tokenized_ngram = stopword_rm.transform(tokenized_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count vectorizer and tfidf\n",
    "cv = CountVectorizer(inputCol='words_nsw', outputCol='tf')\n",
    "cvModel = cv.fit(tokenized_ngram)\n",
    "count_vectorized = cvModel.transform(tokenized_ngram)\n",
    "\n",
    "idf = IDF().setInputCol('tf').setOutputCol('tfidf')\n",
    "tfidfModel = idf.fit(count_vectorized)\n",
    "tfidf_df = tfidfModel.transform(count_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splits = tfidf_df.select(['tfidf', 'label']).randomSplit([0.8,0.2],seed=100)\n",
    "train = splits[0].cache()\n",
    "test = splits[1].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to LabeledPoint vectors\n",
    "train_lb = train.rdd.map(lambda row: LabeledPoint(row[1], MLLibVectors.fromML(row[0])))\n",
    "test_lb = test.rdd.map(lambda row: LabeledPoint(row[1], MLLibVectors.fromML(row[0])))\n",
    "\n",
    "# SVM model\n",
    "numIterations = 50\n",
    "regParam = 0.3\n",
    "svm = SVMWithSGD.train(train_lb, numIterations, regParam=regParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "test_lb = test.rdd.map(lambda row: LabeledPoint(row[1], MLLibVectors.fromML(row[0])))\n",
    "scoreAndLabels_test = test_lb.map(lambda x: (float(svm.predict(x.features)), x.label))\n",
    "score_label_test = spark.createDataFrame(scoreAndLabels_test, [\"prediction\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# F1 score\n",
    "f1_eval = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "svm_f1 = f1_eval.evaluate(score_label_test)\n",
    "print(\"F1 score: %.4f\" % svm_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
